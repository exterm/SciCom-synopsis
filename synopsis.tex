\documentclass[a4paper, 12pt]{article}

\usepackage[margin=2cm,top=2.7cm,bottom=2.7cm]{geometry}
\usepackage[ngerman]{babel}
\usepackage[utf8]{inputenc}
\usepackage{lastpage}
\usepackage[bookmarks, hidelinks]{hyperref}
\usepackage{fancyhdr}
\usepackage{setspace}
\usepackage{comment}
\usepackage{graphicx}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{enumitem}

\setlength{\parindent}{0pt}
\setlist[description]{itemindent=-30pt, leftmargin=50pt, itemsep=.5em}

\pagestyle{fancy}
 
\lhead{Scientific Computing}
\chead{Zusammenfassung}
\rhead{WS 2011/2012}

\lfoot{Philip Müller, inf9293}
\cfoot[Seite \thepage\ von \pageref{LastPage}]{Seite \thepage\ von \pageref{LastPage}}
\rfoot{\today}
\renewcommand{\footrulewidth}{.5pt}

\begin{document}

\tableofcontents
\pagebreak



\part{Grundlagen}



\section{Definition}
Scientific Computing ist
\begin{itemize}
  \item numerische Simulation
  \item Lösung numerischer Probleme aus verschiedenen wissenschaftlichen Disziplinen
\end{itemize}



\section{Numerik-Grundlagen}


\subsection{Reelle Zahlen}

\subsubsection*{Repräsentation}
\begin{itemize}
  \item Im Rechner üblicherweise Teilmenge der reellen Zahlen benutzt: Gleitkommazahlen
  \item Gleitkomma-Repräsentation einer reellen Zahl verlustbehaftet, ``beschnittene Zahl''
\end{itemize}
\begin{description}
  \item[Darstellung] durch
    \begin{itemize}
      \item Vorzeichen
      \item Basis
      \item Mantisse
      \item Exponent
    \end{itemize}
  \item[Rundungsfehler] beim Repräsentieren
    \begin{itemize}
      \item \(|x-fl(x)|/|x| \le \frac{1}{2} \epsilon_M\)
      \item \(\epsilon_M = \beta^{1-t}\) gibt Abstand zwischen 1 und der nächsten Gleitkommazahl größer 1 an
      \item \(\epsilon_M\) in MATLAB durch das Kommando \texttt{eps} bestimmbar
    \end{itemize}
\end{description}

\subsubsection*{Eingeschränkte Gesetze}
\begin{itemize}
  \item Kommutativität gilt
  \item die 0 ist nicht eindeutig
  \item Assoziativität und Distributivität gelten nicht!\\
    z.B. bei Rechnungen die an die Ränder des Zahlenbereichs gehen
\end{itemize}

\subsubsection*{Auslöschen signifikanter Stellen}
Wenn man zwei Gleitkommazahlen addiert, die ähnliche Absolutwerte haben, aber gegenätzliches Vorzeichen, wird das Ergebnis ungenau.

Beispiel:
\begin{align*}
x = 1e-15\\
\Rightarrow \frac{((1+x)-1)}{x} = 1.1102
\end{align*}
11\% Fehler!


\subsection{Komplexe Zahlen}
--- Nur simple Wiederholung und MATLAB ---


\subsection{Matrizen}
--- Nur simple Wiederholung und MATLAB ---


\subsection{Vektoren}
--- Nur simple Wiederholung und MATLAB ---




\part{Methoden}



\section{Nichtlineare Gleichungen}


\subsection{Bisektionsverfahren}

\subsubsection*{Verfahren}
\begin{itemize}
  \item sei \(f\) stetige Funktion über \([a,b]\), \(f(a)\times f(b)<0\)
  \item Also mindestens eine Nullstelle in \([a,b]\)
  \item Bisektion geht von genau einer Nullstelle aus
  \item Algorithmus:
    \begin{enumerate}
      \item teile das Interval in zwei gleich große Teilintervalle
      \item fahre mit dem Interval, dessen Grenzen funktionswerte mit verschiedenen Vorzeichen haben, fort
      \item goto 1
    \end{enumerate}
\end{itemize}
\begin{itemize}
  \renewcommand{\labelitemi}{\(-\)}%
  \item Fehler nicht monoton fallend, die Näherung kann mit weiteren Schritten auch schlechter werden
  \item unabhängig von Verlauf der Funktion, könnte deshalb effizienter sein (Beispiel Gerade)
\end{itemize}
\begin{itemize}
  \renewcommand{\labelitemi}{+}%
  \item einfache Implementierung, betrachtet Funktion als ``Black Box''
\end{itemize}

\subsubsection*{Abbruch}
Erfolgt bei Erreichung gewünschter Toleranz.\\
Anzahl erforderlicher Schritte:
\[k_{min} > log_2(\frac{b-a}{\epsilon})-1\]

\subsubsection*{Konvergenz}
Die Bisektion konvergiert immer.


\subsection{Newton-Verfahren}

\subsubsection*{Verfahren}
\begin{itemize}
  \item Verwendung der Ableitung der Funktion
  \item Algorithmus:
    \begin{enumerate}
      \item Starte mit einem beliebigen Punkt \(x_0\) (\(k=0\))
      \item Bestimme die Tangente \(T\) im Punkt \(x_k\)
      \item \(x_{k+1} = T^{-1}(0)\)
      \item \(k=k+1\), goto 2
    \end{enumerate}
\end{itemize}

\subsubsection*{Abbruch}
Mögliche Kriterien:
\begin{itemize}
  \item nach willkürlich festgelegter Iterationszahl --- keine Aussage über Fehler machbar
  \item Abschätzen des Fehlers durch Differenz zweier aufeinanderfolgender Näherungswerte --- funktioniert ``gut'' bei einfachen Nullstellen
  \item \emph{Residuum} (Funktionswert am Näherungswert) unterschreitet Toleranz --- problematisch, da von Steigung der Funktion in der Nähe der Nullstelle abhängig
\end{itemize}

\subsubsection*{Konvergenz}
\begin{itemize}
  \item konvergiert für lineare Funktionen in einem Schritt
  \item allgemein konvergiert das Verfahren nicht immer, der Startpunkt muss \emph{nahe genug} an der Nullstelle sein --- das ist abhängig von der Funktion
  \item Idee: Mittels einiger Bisektionsschritte nahe genug an die Nullstelle kommen, dann mit Newton verfeinern
\end{itemize}


\subsection{Fixpunkt-Iteration}

\subsubsection*{Verfahren}
\begin{itemize}
  \item Methode zum Finden von Fixpunkten einer Funktion \(f\)
  \item Algorithmus:
    \begin{enumerate}
      \item \(f\) wird auf einen Startwert angewendet
      \item \(f\) wird auf das Ergebnis angewendet
      \item goto 2
    \end{enumerate}
  \item Das Newton-Verfahren lässt sich als Fixpunkt-Iteration über die Funktion
    \[\phi(x) = x-\frac{f(x)}{f'(x)}\]
    darstellen
\end{itemize}

\subsubsection*{Abbruch}
Wenn die absolute Differenz zweier aufeinanderfolgender Näherungen unter einem Toleranzwert liegt. Die Abschätzung des Fehlers nach dieser Methode ist umso genauer, je näher \(|\phi'(\alpha)|\) an 0 liegt.

\subsubsection*{Konvergenz}
Schwierig zu bewerten. Sicher konvergiert die Fixpunkt-Iteration aber, wenn diese Bedingungen erfüllt sind:
\begin{itemize}
  \item Betrachte Funktion \(\phi\) auf einem Intervall \([a,b]\)
  \item \(\phi\) ist in \([a,b]\) differenzierbar
  \item \(\forall x \in [a,b]: \phi(x) \in [a,b]\)
  \item \(\forall x \in [a,b]: |\phi'(x)|<1\)
\end{itemize}
\(\phi\) besitzt dann genau einen Fixpunkt im Intervall \([a,b]\) und die Fixpunkt-Iteration konvergiert unabhängig von der Wahl des Startwerts (der allerdings im Intervall liegen muss).


\subsection{Newton-Horner-Verfahren}

\subsubsection*{Algebraische Polynome}
\begin{itemize}
  \item Nullstellen von Polynomen mit Grad größer 5 sind umständlich zu berechnen
  \item Suche nach effizienter Methode
\end{itemize}

\subsubsection*{Horner-Schema}
Das Horner-Schema beschreibt die Umformung eines Polynoms in eine \emph{geschachtelte Multiplikation}, um das Auswerten zu vereinfachen. Es kann genutzt werden, um die Polynomdivision sowie die Berechnung von Nullstellen und Ableitungen zu vereinfachen.

Das Verfahren zum Auswerten eines Polynoms nach dem Horner-Schema, die \emph{synthetische Division}, erlaubt es, zu einem Polynom \(p_n(x)\) für ein beliebiges \(z\) aus dem Definitionsbereich von \(p_n\) ohne großen Mehraufwand nicht nur \(p_n(z)\), sondern auch \(p_n'(z)\) und \(\frac{p_n(x)}{x-z}\) (das \emph{assoziierte Polynom}) zu berechnen.

Das assoziierte Polynom \(q_{n-1,z}\) lässt sich definieren als das Ergebnis einer Division von \(p_n\) durch \((x-z)\) ohne Rest.

Die Herleitung zu dieser ``Magie'' gibt es in der Ausarbeitung von Ulf Sauer.

Hinweis: Ist \(z\) eine Nullstelle von \(p_n\), so hat das berechnete assoziierte Polynom alle Nullstellen von \(p_n\) außer \(z\) (``Polynomdivision durch Nullstelle'' aus der Analysis bekannt!).

\subsubsection*{Verfahren}
Das Newton-Horner-Verfahren wird genutzt, um alle Nullstellen eines Polynoms \(n\)-ten Grades \(p_n(x)\) zu finden. Idee:
\begin{enumerate}
  \item Berechnen einer Nullstelle \(z\) durch das Newton-Verfahren
  \item Berechnen des assoziierten Polynoms \(q_{n-1,z}\) (hat die restlichen Nullstellen, aber geringeren Grad)
  \item fahre mit \(q_{n-1,z}\) bei Punkt 1 fort, es sei denn, \(q_{n-1,z}\) ist konstant
\end{enumerate}
Dabei muss in jedem Schritt des Newton-Verfahrens an einer Stelle \(x\) sowohl das Polynom als auch seine Ableitung ausgewertet werden (Tangentensteigung). Die synthetische Division kann hier beide Auswertungen übernehmen und dazu gleich noch das assoziierte Polynom liefern, mit dem im Fall einer gefundenen Nullstelle direkt weitergemacht werden kann, um die nächste Nullstelle zu finden.

Die synthetische Division des Horner-Schemas spart hier also reichlich Berechnungsarbeit.



\section{Numerisches Differenzieren}


\subsection{Problem}
Habe Funktionswerte an mehreren Stellen aber keine Funktionsdefinition, möchte Änderungsrate (Ableitung) approximieren.


\subsection{finite Differenzen}
Entfernung der Punkte (\(x\)-Differenzen) sind im vornherein als \(h\) festgelegt.
\begin{description}
  \item[Vorwärtsdifferenz:] Steigung von Sekante durch aktuellen Punkt und Punkt mit größerem \(x\)-Wert
  \item[Rückwärtsdifferenz:] Steigung von Sekante durch aktuellen Punkt und Punkt mit kleinerem \(x\)-Wert
  \item[zentrale finite Differenz:] Steigung von Sekante durch in gleichen \(x\)-Abständen vorwärts und rückwärts gelegene Punkte
\end{description}


\subsection{Taylor-Entwicklung}
Näherungsweise Darstellung einer Funktion, die über die Funktionswerte der ersten n Ableitungen an einer Stelle konstruiert wird.

Damit kann eine Fehlerabschätzung für numerische Differenziationsverfahren definiert werden. Beispiel Vorwärtsdifferenz:
\begin{enumerate}
  \item Taylor-Entwicklung erster Ordnung um \((x+h)\) (ergibt alternative Darstellung für \(f(x+h)\))
  \item Einsetzen in Definition der Vorwärtsdifferenz \(\delta_+f\)
  \item Auflösen nach dem Fehler: \((\delta_+f)(x) - f'(x) = \frac{h}{2}f''(\xi)\)
\end{enumerate}
Also ist die Vorwärtsdifferenz eine Approximation erster Ordnung (linearer Fehler).


\section{Numerisches Integrieren}


\subsection{Problem}
\begin{itemize}
  \item Bestimmung einer Stammfunktion \(F\) zu einer beliebigen Funktion \(f\) nicht möglich
  \item eventuell liegt \(f\) auch garnicht vor, sondern nur ein paar Funktionswerte
  \item numerische (genäherte) Lösung für bestimmtes Intervall gesucht
  \item Idee: stückweise Annäherung von \(f\) durch Polynome
\end{itemize}


\subsection{Mittelpunktformel}

\subsubsection*{Verfahren}
\begin{itemize}
  \item Ablauf:
    \begin{enumerate}
      \item Unterteile Intervall in gleich große Teilintervalle
      \item approximiere \(f\) in jedem Teilintervall durch Konstante: Funktionswert in der Mitte des Teilintervalls
      \item summiere Flächen der Rechtecke aus Teilintervallgröße und Ap\-pro\-xi\-mie\-rungs-Kon\-stan\-te
    \end{enumerate}
\end{itemize}

\subsubsection*{Fehler}
Fehler der Ordnung 2 (quadratischer Fehler) bezüglich der Teilintervallbreite

\subsubsection*{Exaktheitsgrad}
Der Exaktheitsgrad sagt aus, bis zu welchem Grad Polynome mit einem Verfahren exakt integriert werden können.

Die Mittelpunktformel hat einen Exaktheitsgrad von 1. Konstante und lineare Funktionen können exakt integriert werden.


\subsection{Trapezformel}

\subsubsection*{Verfahren}
Wie Mittelpunktformel, aber Approximation der Teilintervalle nicht durch Konstanten sondern Geraden. Dazu werden die Funktionswerte an den Teilintervallgrenzen bestimmt.

\subsubsection*{Fehler}
Ordnung 2, Exaktheitsgrad 1 (wie Mittelpunktformel)


\subsection{Gaußsche Quadraturformel}

\subsubsection*{Verfahren}
Wie Trapezformel, aber Aufteilung des Intervalls nicht äquidistant sondern an Gauss-Knoten

\subsubsection*{Fehler}
Genauigkeitsordnung 4, Exaktheitsgrad 3


\subsection{Simpson-Formel}

\subsubsection*{Verfahren}
Approximation des Teilintervalls durch eine Parabel (Polynom vom Grad 2), die durch die Funktionswerte an den Rändern und in der Mitte des Intervalls gelegt wird.

\subsubsection*{Fehler}
Genauigkeitsordnung 4, Exaktheitsgrad 3



\section{Lineare Systeme}


\subsection{LU-Faktorisierung}
--- hier Text einfügen ---


\subsection{Gauß-Faktorisierung}
--- hier Text einfügen ---


\subsection{Pivoting}
--- hier Text einfügen ---



\section{Gewöhnliche Differentialgleichungen}


\subsection{Allgemeines}
--- hier Text einfügen ---


\subsection{Euler-Verfahren}
--- hier Text einfügen ---


\subsection{Verfahren höherer Ordnung}
--- hier Text einfügen ---



\section{Interpolation}


\subsection{Langrange-Interpolation}
--- hier Text einfügen ---


\subsection{Runge-Interpolation}
--- hier Text einfügen ---


\subsection{Chebyshev-Interpolation}
--- hier Text einfügen ---


\subsection{Stückweise Lineare Interpolation}
--- hier Text einfügen ---


\subsection{Spline-Funktionen}
--- hier Text einfügen ---



\section{Eigenwerte und Eigenvektoren}


\subsection{Potenzverfahren}
--- hier Text einfügen ---


\subsection{QR-Zerlegung}
--- hier Text einfügen ---




\part{Parallelisierung}



\section{IPython}


\subsection{Was ist das?}
\begin{itemize}
  \item Interaktives Rechensystem für parallele Berechnungen
  \item Architektur:
    \begin{itemize}
      \item beliebig viele Engines (mit queues), die Code ausführen
      \item jede Engine gehört genau einem Controller
      \item ein Controller kann beliebig viele Engines haben
      \item ein Client kann beliebig viele Controller steuern
    \end{itemize}
  \item einfache Datenparallelität durch paralleles \texttt{map}
\end{itemize}


\subsection{Befehlsausführung}
\begin{itemize}
  \item \texttt{execute} für explizite Ausführung von Python-Codeschnipseln auf bestimmten Engines
  \item blockierend oder mit Warte-Objekt (\texttt{PendingResult})
  \item \emph{magische} Kommandos: \texttt{\%px} führt ein Kommando parallel auf allen Engines aus
\end{itemize}

\subsection{Objekte kopieren}
\begin{itemize}
  \item mit \texttt{push} und \texttt{pull} können Daten zwischen Client und Engines kopiert werden (deep copy)
  \item Funktionen mit \texttt{push\_function} und \texttt{pull\_function}
  \item verteilte Dictionaries erledigen Verteilungsarbeit automatisch
  \item \texttt{scatter} und \texttt{gather} teilen Listenelemente automatisch auf Engines auf und sammeln die Liste wieder zusammen
\end{itemize}


\subsection{\texttt{Task}-Schnittstelle}
\begin{itemize}
  \item nicht Engine-orientiert
  \item \texttt{map}-Funktionalität mit Load Balancing
  \item Tasks repräsentieren Aufgaben, können einfach im Kontext eines \texttt{TaskClient} gestartet werden
  \item der \texttt{TaskClient} kümmert sich um Art und Ausführung der Verteilung inkl.\ Load Balancing
\end{itemize}



\section{Leistungsmaße}


\subsection{Problemstellung}
Ziele der Parallelisierung:
\begin{itemize}
  \item Reduktion der Laufzeit, kürzere Wartezeit bis zum Ergebnis
  \item genauere Lösung in der gleichen Laufzeit
  \item größere Datenmengen in der gleichen Laufzeit
\end{itemize}
\begin{itemize}
  \renewcommand{\labelitemi}{\Large{?}}%
  \item Messbarkeit des Erfolgs
  \item Mögliche Maße
\end{itemize}


\subsection{Laufzeit}
Messung der Laufzeit eines Programmablaufs auf der Zielhardware
\begin{description}
  \item[Laufzeit]~\\
    Zeit zwischen Start und Ende der Abarbeitung des Programms auf allen beteiligten Prozessoren, zusammengesetzt aus
    \begin{itemize}
      \item Rechenzeit \(T_{CPU}\)
      \item Kommunikationszeit \(T_{COM}\)
      \item Wartezeit \(T_{WAIT}\)
      \item Synchronisationszeit \(T_{SYN}\)
      \item Platzierungszeit \(T_{place}\)
      \item Startzeit \(T_{start}\)
    \end{itemize}
  \item[Overhead-Zeit]~\\
    \(T_{CWS}=T_{COM}+T_{WAIT}+T_{SYN}\)
  \item[Rüstzeit]~\\
    \(T_{setup}=T_{place}+T_{start}\)
\end{description}


\subsection{Speedup}
Reduktion der Laufzeit einer parallelen Lösung bezogen auf die sequentielle Variante
\[
S_p(n) = \frac{T'(n)}{T_p(n)}
\]
\begin{description}
  \item[\(T'(n)\)] Laufzeit des schnellsten bekannten sequentiellen Algorithmus
  \item[\(T_1(n)\)] Laufzeit des parallelen Algorithmus auf einem Prozessor (üblicherweise größer als \(T'(n)\))
  \item[\(S_p(n)=p\)] linearer Speedup
  \item[\(S_p(n)>p\)] superlinearer Speedup
\end{description}
\begin{itemize}
  \item der Speedup ist normalerweise durch die Anzahl der Prozessoren begrenzt (\(S_p(n)\le p\))
  \item der reale Speedup unterscheidet sich vom linearen Speedup um die Overheadzeit \(T_{CWS}\)
\end{itemize}


\subsection{Kosten}
\begin{itemize}
  \item Maß für die von allen Prozessoren durchgeführte Arbeit\\
    \[C_p(n)=T_p(n)\cdot p\]
  \item \emph{kostenoptimal} ist ein paralleles Programm, wenn es genau so viele Operationen ausführt wie das entsprechende sequentielle Programm (Laufzeit \(:=\) Anzahl Operationen)\\
    \[C_p(n)=T'(n)\]
\end{itemize}


\subsection{Kosten-Overhead}
\begin{itemize}
  \item Differenz zwischen den Kosten eines parallelen und des entsprechenden sequentiellen Programms\\
    \[H_p(n)=C_p(n)-T'(n) = p\cdot T_p(n) - T'(n)\]
  \item \((H_p(n)=0)\Rightarrow\) Programm ist kostenoptimal
\end{itemize}


\subsection{Effizienz}
\begin{itemize}
  \item Maß für die Laufzeitverkürzung auf \(p\) Prozessoren
  \item Speedup relativ zu Prozessorzahl\\
    \[E_p(n)=\frac{S_p(n)}{p}=\frac{T'(n)}{p\cdot T_p(n)}=\frac{T'(n)}{C_p(n)}\]
\end{itemize}


\subsection{Amdahls Gesetz}
Definiert obere Schranke für erreichbaren Speedup. Prinzip:
\begin{itemize}
  \item Abschätzung von zwangsweise sequentiellem Anteil \(f\) und parallelisierbarem Anteil \(1-f\)
  \item Aussage über Gesamtlaufzeit:
    \[
    T_p(n)\ge f\cdot T'(n) + \frac{(1-f)\cdot T'(n)}{p}
    \]
  \item Aussage über Speedup:
    \[S_p(n)\le\frac{1}{f+\frac{1-f}{p}}\]
  \item mit beliebig vielen Prozessoren (\(p\rightarrow\infty\)) liesse sich maximal erreichen:
    \[S_p(n)\le \frac{1}{f}\]
  \item Das ist ein theoretischer Wert, Amdahl argumentierte, dass mit zunehmender Prozessorzahl der Speedup sogar irgendwann wieder sinkt, aufgrund übermäßigen Overheads
  \item Amdahl geht von maximal linearem Speedup aus (für 20-fachen Speedup brauche ich mindestens 20 Prozessoren, selbst bei idealer Parallelisierbarkeit, also \(f=0\))
\end{itemize}


\subsection{Gustafsons Gesetz}
Betrachtet Speedup anders: Wie wirkt sich die Anzahl der Prozessoren auf die Problemgröße aus, die in gegebener Zeit abgearbeitet werden kann?
\begin{itemize}
  \item Annahme: der sequentielle Anteil verkleinert sich mit zunehmender Problemgröße
  \item Sei \(f_1\) der sequentielle Anteil bei einer feste Problemgröße und der Bearbeitung auf einem Prozessor, dann gilt laut Gustafson:
    \[f=\frac{f_1}{p\cdot (1-f_1) +f_1}\]
    Mit Amdahls Gesetz folgt
    \[S_p(n)\le p\cdot (1-f_1)+f_1\]
    Unter der Voraussetzung, dass die Gesamtlaufzeit gleich bleibt und die Problemgröße entsprechend mit der Zahl der Prozessoren steigt, ist also der Speedup durch Erhöhung der Prozessorenzahl nahezu linear.
\end{itemize}


\subsection{Skalierbarkeit}
Eine Anwendung heisst \emph{skalierbar}, wenn die zur Problemlösung benötigte Zeit bei wachsender Problemgröße durch Erhöhung der Zahl der beteiligten Prozessoren konstant gehalten werden kann.



\section{Zerlegungsmethoden}


\subsection{Allgemeines}
--- hier Text einfügen ---


\subsection{Funktionale Zerlegung}
--- hier Text einfügen ---


\subsection{Datenzerlegung}
--- hier Text einfügen ---


\subsection{Funktions- und Datenzerlegung}
--- hier Text einfügen ---



\section{Beispiele}


\subsection{Parallele LU-Faktorisierung}
--- hier Text einfügen ---






%%%%%%%%%%



\section{Bli}


\subsection{bla}

\subsubsection*{blubb}

\begin{itemize}
  \item allgemeine Eigenschaften
\end{itemize}
\begin{itemize}
  \renewcommand{\labelitemi}{\(-\)}%
  \item Nachteile
\end{itemize}
\begin{itemize}
  \renewcommand{\labelitemi}{+}%
  \item Vorteile
\end{itemize}



\end{document}
